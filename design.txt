A network consists of layers.

Associated with each layer is a forward function that transforms its
input data to its output data using zero or more parameters.

Associated with each layer is a backward function that takes the
gradient of its output and computes the gradient of its input, and
gradients of its parameters if any.


Examples: 

type	params	y		dw		dx
----	------	--		--		--
linw	w	w*x		dy*x'		w'*dy
linb	w,b	w*x+b		dy*x',Î£dy	w'*dy
relu	--	x.*(x>0)	--		dy.*(dy>0)
soft	--	softmax(x)	--		(y-Y)/size(y,2)
drop
sprs

where x is input, y is output, w* are parameters, d* are gradients for
one layer.  Y is the desired final outputs of the network.


Notes:

- we need to remember all x's input to linw layers during forward to
  compute dw during backprop.

- the x's input to relu and soft don't get used by backprop, so we can
  optimize code by overwriting them with the output.

- the dy/dx only gets used once, so no need to keep a list of them
  around, we can just keep one.

